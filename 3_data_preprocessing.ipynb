{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utilspro.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Displaying the first few rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning Steps:\n",
    "- Handle placeholder values (?).\n",
    "- Convert date columns to the proper datetime format.\n",
    "- Address the inconsistency with incidents marked as active but having a closed_at date.\n",
    "- Handle any other anomalies and inconsistencies identified during the EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling placeholder values (?)\n",
    "We previously identified several columns containing the placeholder value ?. We'll replace these placeholders with appropriate NaN (null) values, which will allow us to handle them more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing '?' with NaN\n",
    "data.replace('?', pd.NA, inplace=True)\n",
    "\n",
    "# Checking the number of missing values in each column after replacement\n",
    "missing_values = data.isna().sum()\n",
    "\n",
    "missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Based imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the KNN imputer may generate new labels not seen during the initial label encoding, we need to ensure that, post-imputation, only the known labels are used for inverse transformation.\n",
    "\n",
    "strategy:\n",
    "\n",
    "Using the KNN imputer on the data as before.\n",
    "Post-imputation, for the columns that were label-encoded, clip any values that lie outside the range [0, number of classes for that column - 1].\n",
    "Use inverse transformation on these clipped values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_cleaned_updated=missing_values[missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Selecting columns with missing values\n",
    "columns_with_missing = missing_values_cleaned_updated.index.tolist()\n",
    "\n",
    "# Creating a subset of data with these columns\n",
    "data_missing = data.copy()[columns_with_missing]\n",
    "\n",
    "# Label Encoding for categorical variables\n",
    "label_encoders = {}\n",
    "for col in columns_with_missing:\n",
    "    if data_missing[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        data_missing[col] = le.fit_transform(data_missing[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Scaling the data for KNN\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_missing)\n",
    "\n",
    "# Scaling the data for KNN\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_missing)\n",
    "\n",
    "# KNN Imputer initialization and imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "data_imputed = knn_imputer.fit_transform(data_scaled)\n",
    "\n",
    "# Convert imputed data back to dataframe\n",
    "data_imputed_df = pd.DataFrame(data_imputed, columns=columns_with_missing)\n",
    "\n",
    "# Clipping values for label encoded columns to ensure they lie within the known labels range\n",
    "for col, le in label_encoders.items():\n",
    "    max_label = len(le.classes_) - 1\n",
    "    data_imputed_df[col] = data_imputed_df[col].clip(0, max_label).astype(int)\n",
    "\n",
    "    # Inverse transform for label encoded columns\n",
    "    data_imputed_df[col] = le.inverse_transform(data_imputed_df[col])\n",
    "\n",
    "# Checking if missing values are imputed and if any unknown labels were introduced\n",
    "missing_after_imputation = data_imputed_df.isna().sum()\n",
    "\n",
    "missing_after_imputation\n",
    "#check the number of columns before and after imputation\n",
    "print('Number of columns before imputation: ', data.shape[1])\n",
    "print('Number of columns after imputation: ', data_imputed_df.shape[1])\n",
    "\n",
    "# Replacing the columns in the original dataset with the imputed versions\n",
    "data_cleaning = data.copy()\n",
    "for col in columns_with_missing:\n",
    "    data_cleaning[col] = data_imputed_df[col]\n",
    "\n",
    "# Checking the number of columns after the replacement\n",
    "num_columns_after_replacement = data_cleaning.shape[1]\n",
    "\n",
    "print('Number of columns after replacement', num_columns_after_replacement)\n",
    "\n",
    "data_cleaning.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting date columns to datetime format\n",
    "date_columns = ['opened_at', 'sys_created_at', 'resolved_at', 'closed_at']\n",
    "for column in date_columns:\n",
    "    data_cleaning[column] = data_cleaning[column].apply(robust_date_parser)\n",
    "\n",
    "# Checking the datatypes of the columns after conversion\n",
    "data_cleaning[date_columns].dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's address the anomalies and inconsistencies we identified during our exploratory data analysis (EDA):\n",
    "\n",
    "- Incidents with closed_at dates but marked as active: \n",
    "We found 116,726 such incidents. This is inconsistent since active incidents should not have a closure date.\n",
    "- Potential Outliers:\n",
    "Reassignment Count: Some incidents have been reassigned more than 20 times.\n",
    "- Reopen Count: \n",
    "Some incidents have been reopened multiple times.\n",
    "- Sys Mod Count: \n",
    "Some incidents have more than 40 system modifications.\n",
    "\n",
    "### Addressing Anomalies:\n",
    "- Incidents with closed_at dates but marked as active:\n",
    "### Our solution: Set these incidents as inactive (active = False).\n",
    "\n",
    "- Potential Outliers:\n",
    "For each of the columns (Reassignment Count, Reopen Count, Sys Mod Count), we can:\n",
    "a. Cap the values at a certain threshold based on domain knowledge or statistical measures (like the 95th percentile).\n",
    "b. Investigate further to understand the reasons for such high values.\n",
    "c. Leave them as they are if they represent genuine scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting incidents with 'closed_at' dates but marked as 'active' to inactive\n",
    "data_cleaning.loc[(data_cleaning['active'] == True) & (data_cleaning['closed_at'].notna()), 'active'] = False\n",
    "\n",
    "# Checking the number of incidents that are still marked as 'active' but have a 'closed_at' date\n",
    "active_with_closed_date = data_cleaning[(data_cleaning['active'] == True) & (data_cleaning['closed_at'].notna())].shape[0]\n",
    "\n",
    "active_with_closed_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Outliers:\n",
    "\n",
    "For columns : Reassignment Count, Reopen Count, and Sys Mod Count, we can cap values beyond the 0.95 quantile threshold as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = ['reassignment_count', 'reopen_count', 'sys_mod_count']\n",
    "for column in columns_list:\n",
    "    threshold = data_cleaning[column].quantile(0.95)\n",
    "    data_cleaning[column] = data_cleaning[column].apply(lambda x: threshold if x > threshold else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "- Date Features: Extract relevant information from date columns.\n",
    "Day of the week, hour, and month from the opened_at column.\n",
    "Time taken to resolve an incident (difference between closed_at and opened_at).\n",
    "\n",
    "- Categorical Features: Convert categorical variables into a format suitable for machine learning models.\n",
    "One-hot encode categorical columns like incident_state, contact_type, and priority.\n",
    "For high cardinality categorical columns, consider using target encoding or other encoding techniques.\n",
    "\n",
    "- Text Features: If there are textual descriptions or notes in the dataset, derive features from them.\n",
    "Text length, sentiment analysis, or even more advanced techniques like TF-IDF or embeddings (this would depend on the nature and quality of the text data).\n",
    "\n",
    "- Interaction Features: Create interaction terms between relevant features, which can sometimes capture patterns that individual features might miss.\n",
    "\n",
    "- Normalization: Depending on the model we decide to use later, we might need to normalize or standardize some numerical features.\n",
    "\n",
    "Let's start by extracting features from the date columns, specifically from the opened_at column. We'll derive the day of the week, hour, and month from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from the 'opened_at' column\n",
    "data_cleaning['opened_day_of_week'] = data_cleaning['opened_at'].dt.dayofweek\n",
    "data_cleaning['opened_hour'] = data_cleaning['opened_at'].dt.hour\n",
    "data_cleaning['opened_month'] = data_cleaning['opened_at'].dt.month\n",
    "\n",
    "# Calculating the resolution time in hours (if it hasn't been calculated already)\n",
    "if 'resolution_time' not in data_cleaning.columns:\n",
    "    data_cleaning['resolution_time'] = (data_cleaning['closed_at'] - data_cleaning['opened_at']).dt.total_seconds() / (60 * 60)\n",
    "\n",
    "# Displaying the first few rows with the new features\n",
    "data_cleaning[['opened_at', 'opened_day_of_week', 'opened_hour', 'opened_month', 'resolution_time']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the resolution duration in hours\n",
    "data_cleaning['resolution_duration'] = (data_cleaning['closed_at'] - data_cleaning['opened_at']).dt.total_seconds() / 3600\n",
    "\n",
    "# Displaying the first few rows with the 'resolution_duration' feature\n",
    "data_cleaning[['opened_at', 'closed_at', 'resolution_duration']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking for duplicated entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "data_cleaning.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the duplicated rows and make a list out  of the 'number' column values\n",
    "duplicated_rows = data_cleaning[data_cleaning.duplicated(keep=False)].sort_values(by='number')\n",
    "duplicated_rows['number'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe with the duplicated rows stating the number of duplicates per unique number value\n",
    "duplicated_rows_count = duplicated_rows.groupby('number').size().reset_index(name='count')\n",
    "# plot the most numeros duplicated rows count in a descending order\n",
    "duplicated_rows_count.sort_values(by='count', ascending=False).head(10).plot.bar(x='number', y='count', figsize=(10, 6))\n",
    "#label the plot\n",
    "plt.title('Number of duplicates per unique number value')\n",
    "plt.xlabel('Number')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the duplicates for number 'INC0019396' order by duplicated sequence  \n",
    "duplicated_rows[duplicated_rows['number'] == 'INC0019396'].sort_values(by='sys_created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the duplicates\n",
    "data_cleaning.drop_duplicates(inplace=True)\n",
    "# check the number of rows after dropping the duplicates\n",
    "data_cleaning.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the number column\n",
    "data_cleaning.drop('number', axis=1, inplace=True)\n",
    "# check the aspect of all categorical columns uniques values count\n",
    "data_cleaning.select_dtypes('object').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utilspro.py\n",
    "encoder = SmartEncoder()\n",
    "# Encoding the categorical columns with the SmartEncoder object the target column is 'acti\n",
    "encoded_df = encoder.fit_transform(data_cleaning, target_column='active')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first few rows of the encoded dataframe\n",
    "encoded_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Exclude datetime columns and target column\n",
    "X = encoded_df.drop(['opened_at', 'sys_created_at', 'resolved_at', 'closed_at', 'active'], axis=1)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Original number of features: {X_scaled.shape[1]}\")\n",
    "print(f\"Reduced number of features: {X_pca.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chenck the number of components\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use variance ratio to check the explained variance of the components\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bypass_columns = ['number', 'opened_at', 'sys_created_at', 'resolved_at', 'closed_at', 'resolution_time', 'resolution_duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a longitudinal correlation funnel for the target column 'active'\n",
    "# write a function to plot the correlation funnel\n",
    "\n",
    "def plot_correlation_funnel(data, target_column, figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Plots a correlation funnel for the target column of a dataframe\n",
    "    \"\"\"\n",
    "    # Calculate the correlation coefficients\n",
    "    corr = data.corr()[target_column].sort_values(ascending=False)\n",
    "\n",
    "    # Exclude the target column\n",
    "    corr = corr.drop(target_column)\n",
    "\n",
    "    # Calculate the number of features\n",
    "    num_features = corr.shape[0]\n",
    "\n",
    "def correlation_funnel(df, target_column, corr_threshold=0.1, multicollinearity_threshold=0.8):\n",
    "    # Step 1 & 2: Calculate correlation with target and filter\n",
    "    corr_with_target = df.drop(columns=target_column).apply(lambda x: x.corr(df[target_column]))\n",
    "    significant_features = corr_with_target[corr_with_target.abs() > corr_threshold].index.tolist()\n",
    "    \n",
    "    # Step 3: Check for multicollinearity\n",
    "    corr_matrix = df[significant_features].corr().abs()\n",
    "    \n",
    "    # Create a mask for the upper triangle of the correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    \n",
    "    # Find features with correlation greater than the threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > multicollinearity_threshold)]\n",
    "    \n",
    "    # Drop features \n",
    "    significant_features = [feat for feat in significant_features if feat not in to_drop]\n",
    "    \n",
    "    return significant_features\n",
    "\n",
    "# Example usage:\n",
    "# selected_features = correlation_funnel(df, 'target_column_name')\n",
    "\n",
    "    # Plot the funnel\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.barh(range(num_features), corr)\n",
    "    plt.yticks(np.arange(num_features), corr.index.tolist())\n",
    "    plt.title('Correlation Funnel')\n",
    "    plt.xlabel('Correlation Coefficient')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "%run utilspro.py\n",
    "#code active's True as 1 and False as 0\n",
    "data_cleaning['active'] = data_cleaning['active'].astype(int)\n",
    "# Plot the correlation funnel for the target column 'active' using numerical columns only\n",
    "plot_correlation_funnel(data_cleaning.select_dtypes('number'), 'active')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
